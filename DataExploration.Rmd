---
title: "DataExploration"
author: "Vix Talbot and Haziel Garcia Sanchez"
date: "2024-02-23"
output: html_document
---
Data Description:

The College data set contains a number of variables for 777 different universities and colleges in the US. 
The variables are:

• Private : Public/private indicator
• Apps : Number of applications received
• Accept : Number of applicants accepted
• Enroll : Number of new students enrolled
• Top10perc : New students from top 10 % of high school class 
• Top25perc : New students from top 25 % of high school class 
• F.Undergrad : Number of full-time undergraduates
• P.Undergrad : Number of part-time undergraduates
• Outstate : Out-of-state tuition
• Room.Board : Room and board costs
• Books : Estimated book costs
• Personal : Estimated personal spending
• PhD : Percent of faculty with Ph.D.’s
• Terminal : Percent of faculty with terminal degree 
• S.F.Ratio : Student/faculty ratio
• perc.alumni : Percent of alumni who donate
• Expend : Instructional expenditure per student
• Grad.Rate : Graduation rate

We would like to predict the number of applications received using the other variables.

```{r setup, include=FALSE}
# load and examine the data set
library(ISLR)
library(ISLR2)
library(skimr)
library(ggplot2)
library(ISLR)
library(boot)
library(tree)
library(caret)
set.seed(1)

data(College)
head(College)
skim(College)

# double check that our data seems to be clean
missing_values <- is.na(College)
missing_counts <- colSums(missing_values)
print(missing_counts)

# View(College)

```

```{r}
# check for which predictors are most important so we can remove unnecessary columns
college_lm <- lm(Apps ~ ., data=College)
summary(college_lm)

# remove P. under, books, personal, terminal, s.f. ratio, perc. alumni
College <- College[-c(8,11,12,14,15,16)]
View(College)

```
```{r}
# confirm that R-squared is still good
reduced_lm <- lm(Apps ~ ., data=College)
summary(reduced_lm)

```


# Creating the training and test set
```{r, echo = FALSE}
#Randomly assigns true or false to each row in the college data set, if true row goes into the train data set, otherwise in the test data set
train = sample(c(TRUE, FALSE), nrow(College), rep = TRUE)
test = (!train)

h.train.college = College[train,]
h.test.college = College[test,]
```


```{r, echo = FALSE}

h.lm.college = lm(Apps~., data = h.train.college)
h.lm.loocv = cv.glm(h.train.college, h.lm.college)$delta[1]

h.yhat = predict(h.lm.college, newdata = h.test.college)
h.predict.mse = mean((h.yhat - h.test.college$Apps)^2)

print(paste("LOOCV estimate of the test error for linear regression is", h.predict.mse))
```


```{r}
# create a 70/30 split to the data
train_indices <- sample(1:nrow(College), (nrow(College) * .7))
train <- College[train_indices, ]
test <- College[-train_indices, ]
```



```{r}
# fit a tree to the data and summarize the results.
library(tree)

college_tree <- tree(Apps ~ ., data = train)
summary(college_tree)
```


```{r}
# display the tree graphically
plot(college_tree, type = "uniform")
text(college_tree)

# compute yhat values and graph
unpruned_yhat <- predict(college_tree, newdata= test)
unpruned_data <- data.frame(unpruned_yhat, test$Apps)
ggplot(unpruned_data, aes(x = unpruned_yhat, y = test.Apps)) +
  geom_point(color = "blue") +
  geom_smooth(method = "lm", se = FALSE, color = "salmon") +
  labs(x = "Y-hat value for unpruned tree", y = "Cross-Validation Errors") +
  theme_minimal()
# MSE for unpruned tree
unpruned_MSE <- mean((unpruned_yhat - test$Apps)^2)
print(paste("The MSE for an unpruned decision tree is", unpruned_MSE))

```

```{r}
# use LOOCV to determine whether pruning is helpful and determine the optimal size for the pruned tree.
tree_cv <- cv.tree(college_tree)
names(tree_cv)
tree_cv

tree_data <- data.frame(Tree_Size = tree_cv$size, Cross_Validation_Errors = tree_cv$dev)

# pretty graphs for presentation
ggplot(tree_data, aes(x = Tree_Size, y = Cross_Validation_Errors)) +
  geom_point(color = "springgreen4", size = 3) +  # Customize points
  geom_line(color = "orange") +  # Add a line connecting points
  labs(x = "Tree Size", y = "Cross-Validation Errors") +  # Customize axis labels
  theme_minimal()  # Apply a minimal theme for a clean appearance
```

```{r}
# compare the pruned and un-pruned trees. Report MSE for the pruned tree. Which predictors seem to be the most important?
pruned_college <- prune.tree(college_tree, best =8)
plot(pruned_college, type = "uniform")
text(pruned_college, pretty = 0)

summary(pruned_college)
```

```{r}
# compare the pruned and un-pruned trees. Report MSE for the pruned tree. Which predictors seem to be the most important?
pruned_yhat <- predict(pruned_college, newdata= test)

# putting some ggplot2 graphs in to start using for our presentation
pruned_data <- data.frame(pruned_yhat, test$Apps)
ggplot(pruned_data, aes(x = pruned_yhat, y = test.Apps)) +
  geom_point(color = "purple") +
  geom_smooth(method = "lm", se = FALSE, color = "cyan") +
  labs(x = "Y-hat value for pruned tree", y = "Cross-Validation Errors") +
  theme_minimal()
# MSE for pruned tree
pruned_MSE <- mean((pruned_yhat - test$Apps)^2)
print(paste("The MSE for the pruned decision tree is", pruned_MSE))
 
```
> So far the MSE is identical for the pruned and unpruned trees because we weren't able to get a better fit through the pruning process. The predictors that appear to be important are Accept and Top10Percent.

```{r}
# use a bagging approach to analyze the data with B = 500 Compute the MSE. Which predictors seem to be the most important?

# bagging is just a randomforest where m = p, mtry = 11 so we use 11 predictors
library(randomForest)
bag500_college <- randomForest(Apps ~ ., data = train, mtry = 11, ntree = 500)

summary(bag500_college)

bag500_yhat <- predict(bag500_college, newdata=test)

# putting some ggplot2 graphs in to start using for our presentation
bagged500_data <- data.frame(bag500_yhat, test$Apps)
ggplot(bagged500_data, aes(x = bag500_yhat, y = test.Apps)) +
  geom_point(color = "magenta") +
  geom_smooth(method = "lm", se = FALSE, color = "royalblue") +
  labs(x = "Y-hat for B = 500", y = "Cross-Validation Errors") +
  theme_minimal()
# MSE for B = 500 bagged tree
bagged500_MSE <- mean((bag500_yhat - test$Apps)^2)
print(paste("The MSE for a bagged decision tree where B = 500 is", bagged500_MSE))

```

```{r}
# plot showing fit for bagged 500 model

predictions <- predict(bag500_college, newdata =test)
plot_data <- data.frame(Actual = test$Apps, Predicted = predictions)

ggplot(plot_data, aes(x = Actual, y = Predicted)) +
  geom_point(color = "black") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "magenta3") +
  labs(x = "Actual Applications", y = "Predicted Applications") +
  ggtitle("Actual vs. Predicted Applications (Bagged Tree Model B = 500)") +
  theme_minimal()

```

```{r}
# use a bagging approach to analyze the data with B = 1000
bag1000_college <- randomForest(Apps ~ ., data = train, mtry = 11, ntree = 1000)

summary(bag1000_college)

bag1000_yhat <- predict(bag1000_college, newdata=test)

# putting some ggplot2 graphs in to start using for our presentation
bagged1000_data <- data.frame(bag1000_yhat, test$Apps)
ggplot(bagged1000_data, aes(x = bag1000_yhat, y = test.Apps)) +
  geom_point(color = "magenta") +
  geom_smooth(method = "lm", se = FALSE, color = "royalblue") +
  labs(x = "Y-hat for B = 1000", y = "Cross-Validation Errors") +
  theme_minimal()
# MSE for B = 1000 bagged
bagged1000_MSE <- mean((bag1000_yhat - test$Apps)^2)
print(paste("The MSE for a bagged decision tree where B = 1000 is", bagged1000_MSE))

```


```{r}
# plot showing fit for bagged 500 model

predictions <- predict(bag1000_college, newdata =test)
plot_data <- data.frame(Actual = test$Apps, Predicted = predictions)

ggplot(plot_data, aes(x = Actual, y = Predicted)) +
  geom_point(color = "black") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "magenta3") +
  labs(x = "Actual Applications", y = "Predicted Applications") +
  ggtitle("Actual vs. Predicted Applications (Bagged Tree Model B = 1000)") +
  theme_minimal()

```

```{r}
# use a random forest approach to analyze the data with B = 500
rf500_college <- randomForest(Apps ~ ., data = train, mtry = 3, ntree = 500)

summary(rf500_college)

rf500_yhat <- predict(rf500_college, newdata=test)

# putting some ggplot2 graphs in to start using for our presentation
rf500_data <- data.frame(rf500_yhat, test$Apps)
ggplot(rf500_data, aes(x = rf500_yhat, y = test.Apps)) +
  geom_point(color = "magenta") +
  geom_smooth(method = "lm", se = FALSE, color = "royalblue") +
  labs(x = "Y-hat for Random Forest B = 500", y = "Cross-Validation Errors") +
  theme_minimal()
# MSE for Random Forest B = 500
rf500_MSE <- mean((rf500_yhat - test$Apps)^2)
print(paste("The MSE for a random forest appraoch with # trees = 500 is", rf500_MSE))

```


```{r}
# plot showing fit for bagged 500 model

predictions <- predict(rf500_college, newdata =train)
plot_data <- data.frame(Actual = train$Apps, Predicted = predictions)

ggplot(plot_data, aes(x = Actual, y = Predicted)) +
  geom_point(color = "black") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "magenta3") +
  labs(x = "Actual Applications", y = "Predicted Applications") +
  ggtitle("Actual vs. Predicted Applications (Random Forest Tree Model B = 500)") +
  theme_minimal()

```

```{r}
# use a random forest approach to analyze the data with B = 1000
rf1000_college <- randomForest(Apps ~ ., data = train, mtry = 3, ntree = 1000)

summary(rf1000_college)

rf1000_yhat <- predict(rf1000_college, newdata=test)

# putting some ggplot2 graphs in to start using for our presentation
rf1000_data <- data.frame(rf1000_yhat, test$Apps)
ggplot(rf1000_data, aes(x = rf1000_yhat, y = test.Apps)) +
  geom_point(color = "magenta") +
  geom_smooth(method = "lm", se = FALSE, color = "royalblue") +
  labs(x = "Y-hat for Random Forest B = 1000", y = "Cross-Validation Errors") +
  theme_minimal()
# MSE for Random Forest B = 1000
rf1000_MSE <- mean((rf1000_yhat - test$Apps)^2)
print(paste("The MSE for a random forest appraoch with # trees = 1000 is", rf1000_MSE))

```


```{r}
# plot showing fit for random forest 1000 model

predictions <- predict(rf1000_college, newdata =test)
plot_data <- data.frame(Actual = test$Apps, Predicted = predictions)

ggplot(plot_data, aes(x = Actual, y = Predicted)) +
  geom_point(color = "blue") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "magenta3") +
  labs(x = "Actual Applications", y = "Predicted Applications") +
  ggtitle("Actual vs. Predicted Applications (Random Forest Tree Model B = 1000)") +
  theme_minimal()

```

```{r}

# boosting  model
library(gbm)
boost_college <- gbm(Apps ~ ., data = train,
distribution = "gaussian", n.trees = 50, interaction.depth =7)

summary(boost_college)

```
```{r}

boost_yhat <- predict(boost_college, newdata=test)

# putting some ggplot2 graphs in to start using for our presentation
boost_data <- data.frame(boost_yhat, test$Apps)
ggplot(boost_data, aes(x = boost_yhat, y = test.Apps)) +
  geom_point(color = "magenta") +
  geom_smooth(method = "lm", se = FALSE, color = "royalblue") +
  labs(x = "Y-hat for Boosting # Trees = 50", y = "Cross-Validation Errors") +
  theme_minimal()
# MSE for Boosting with 50 trees
boost_MSE <- mean((boost_yhat - test$Apps)^2)
print(paste("The MSE for a boosting model with 50 trees is", boost_MSE))

```


```{r}
# plot showing fit for boost model

predictions <- predict(boost_college, newdata =test)
plot_data <- data.frame(Actual = test$Apps, Predicted = predictions)

ggplot(plot_data, aes(x = Actual, y = Predicted)) +
  geom_point(color = "cyan3") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "magenta3") +
  labs(x = "Actual Applications", y = "Predicted Applications") +
  ggtitle("Actual vs. Predicted Applications (Boosting Model)") +
  theme_minimal()

```

```{r}
library(BART)
x <- College[, -c(2)]
y <- College[, "Apps"]
xtrain <- x[train_indices, ]
ytrain <- y[train_indices]
xtest <- x[-train_indices, ]
ytest <- y[-train_indices]
bartfit <- gbart(xtrain, ytrain, x.test = xtest)
yhat.bart <- bartfit$yhat.test.mean 
bartMSE <- mean((ytest - yhat.bart)^2)
print(paste("The MSE for a BART model is", bartMSE))

```

```{r}

bart_data <- data.frame(yhat.bart, test$Apps)
ggplot(bart_data, aes(x = yhat.bart, y = test.Apps)) +
  geom_point(color = "magenta") +
  geom_smooth(method = "lm", se = FALSE, color = "royalblue") +
  labs(x = "Y-hat for BART", y = "Cross-Validation Errors") +
  theme_minimal()

```


```{r}
# compare the results from the various methods. Which method would you recommend?
MSE_vector <- c(unpruned_MSE, pruned_MSE, bagged500_MSE, bagged1000_MSE, rf500_MSE, rf1000_MSE, boost_MSE, h.predict.mse, bartMSE)
y_values <- seq(0, 20000, length.out = length(MSE_vector))
df <- data.frame(Method = c("Unpruned", "Pruned", "bagged500", "bagged1000", "RF500", "RF1000", "Boost", "Linear Model", "BART"),
                 MSE = MSE_vector)

# comparison of MSE value for various methods
ggplot(df, aes(x = Method, y = MSE, label = Method)) +
  geom_point(color = "blueviolet", size = 3) +
  geom_text(vjust = -0.5, size = 3) +  
  labs(x = "Methods", y = "MSE", title = "MSE Comparison") +
  theme_minimal()

```
> It appears that the Bagged model with B = 500 yields the lowest MSE for our data.

```{r}

# compare our actual error rates
error_vector <- c(sqrt(unpruned_MSE), sqrt(pruned_MSE), sqrt(bagged500_MSE), sqrt(bagged1000_MSE), sqrt(rf500_MSE), sqrt(rf1000_MSE), sqrt(boost_MSE), sqrt(h.predict.mse), sqrt(bartMSE))
error_values <- seq(0, 20000, length.out = length(error_vector))
df <- data.frame(Method = c("Unpruned", "Pruned", "bagged500", "bagged1000", "RF500", "RF1000", "Boost", "Linear Model", "BART"),
                 model_error = error_vector)

# comparison of MSE value for various methods
ggplot(df, aes(x = Method, y = model_error, label = Method)) +
  geom_point(color = "forestgreen", size = 3) +
  geom_text(vjust = -0.5, size = 3) +  
  labs(x = "Methods", y = "Model Error", title = "Model Comparion") +
  theme_minimal()


names <- c("Unpruned", "Pruned", "bagged500", "bagged1000", "RF500", "RF1000", "Boost", "Linear Model", "BART")
results <- paste(names, "MSE:", round(MSE_vector, 2), ", Error Rate:", round(error_vector, 2))

print(results)

```
